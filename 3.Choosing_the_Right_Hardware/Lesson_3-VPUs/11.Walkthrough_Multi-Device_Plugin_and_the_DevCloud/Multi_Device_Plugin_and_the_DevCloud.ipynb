{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_gkm5l7l"
   },
   "source": [
    "# Walkthrough: Multi Device Plugin and the DevCloud\n",
    "\n",
    "This notebook is a demonstration showing you how to request an edge node with an Intel i5 CPU and load a model on the CPU, GPU, and VPU (IntelÂ® Neural Compute Stick 2) at the same time using the Multi Device Plugin on Udacity's workspace integration with Intel's DevCloud. This notebook is just to give you an overview of the process (you won't be writing any code). In the next workspace, you'll be given TODO items to complete. \n",
    "\n",
    "Below are the six steps we'll walk through in this notebook:\n",
    "\n",
    "1. Creating a Python script to load the model\n",
    "2. Creating a job submission script\n",
    "3. Submitting a job using the `qsub` command\n",
    "4. Checking the job status using the `liveQStat` function\n",
    "5. Retrieving the output files using the `getResults` function\n",
    "6. Viewing the resulting output\n",
    "\n",
    "Click the **Introduction to Multi Device Plugin and the DevCloud** button below for a quick overview of the overall process. We'll then walk through each step of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_u5l9o8a"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_u5l9o8a-id_9e2xr8h\"><i></i><button>Introduction to Multi Device Plugin and the DevCloud</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_e6xgsrf"
   },
   "source": [
    "#### IMPORTANT: Set up paths so we can run Dev Cloud utilities\n",
    "You *must* run this every time you enter a Workspace session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_5xxbqx3"
   },
   "outputs": [],
   "source": [
    "%env PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_bqp4w5p"
   },
   "source": [
    "## The Model\n",
    "\n",
    "We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise.\n",
    "\n",
    "Remember to use the appropriate model precisions for each device:\n",
    "\n",
    "* IGPU - `FP16`\n",
    "* VPU - `FP16`\n",
    "* CPU - It is prefered to use `FP32`, but we have to use `FP16` since **GPU** and **VPU** use `FP16`\n",
    "\n",
    "The model has already been downloaded for you in the `/data/models/intel` directory on Intel's DevCloud.\n",
    "\n",
    "We will be running inference on an image of a car. The path to the image is `/data/resources/car.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_7m6znqc"
   },
   "source": [
    "# Step 1: Creating a Python Script\n",
    "\n",
    "The first step is to create a Python script that you can use to load the model and perform an inference. I have used the `%%writefile` magic command to create a Python file called `load_model_to_device.py`. This will create a new Python file in the working directory.\n",
    "\n",
    "**Note**:  The advantage of using the **Multi device plugin** is that it does not require us to change our application code. So we will be using the same Python script we used in the previous VPU walkthrough.\n",
    "\n",
    "Click the **Writing a Python Script** button below for a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_i2z9e6u"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_i2z9e6u-id_yqhna2v\"><i></i><button>Writing a Python Script</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_pwsuvie"
   },
   "outputs": [],
   "source": [
    "%%writefile load_model_to_device.py\n",
    "\n",
    "import time\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IEPlugin\n",
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    model=args.model_path\n",
    "    model_weights=model+'.bin'\n",
    "    model_structure=model+'.xml'\n",
    "    \n",
    "    start=time.time()\n",
    "    model=IENetwork(model_structure, model_weights)\n",
    "\n",
    "    plugin = IEPlugin(device=args.device)\n",
    "    \n",
    "    net = plugin.load(network=model, num_requests=1)\n",
    "    print(f\"Time taken to load model = {time.time()-start} seconds\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', required=True)\n",
    "    parser.add_argument('--device', default=None)\n",
    "    \n",
    "    args=parser.parse_args() \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_pu1wxhq"
   },
   "source": [
    "## Step 2: Creating a Job Submission Script\n",
    "\n",
    "To submit a job to the DevCloud, we need to create a shell script. Similar to the Python script above, I have used the `%%writefile` magic command to create a shell script called `load_multi_model_job.sh`.\n",
    "\n",
    "This script does a few things.\n",
    "1. Writes stdout and stderr to their respective .log files\n",
    "2. Creates the `/output` directory\n",
    "3. Creates `DEVICE ` and `MODELPATH` variables and assigns their value as the first and second argument passed to the shell script\n",
    "4. Calls the Python script using the `MODELPATH` and `DEVICE` variable values as the command line argument\n",
    "5. Changes to the `/output` directory\n",
    "6. Compresses the stdout.log and stderr.log files to `output.tgz`\n",
    "\n",
    "**Note**: Just like our Python script, our job submission script also does not need to change when using the **Multi device plugin**. Step 3, where we submit our job to the DevCloud, is where we have to make a minor change.\n",
    "\n",
    "Click the **Creating a Job Submission Script** button below for a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_2c3xj24"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_2c3xj24-id_z7gc9v1\"><i></i><button>Creating a Job Submission Script</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_duqob4x"
   },
   "outputs": [],
   "source": [
    "%%writefile load_multi_model_job.sh\n",
    "\n",
    "exec 1>/output/stdout.log 2>/output/stderr.log\n",
    "\n",
    "mkdir -p /output\n",
    "\n",
    "DEVICE=$1\n",
    "MODELPATH=$2\n",
    "\n",
    "# Run the load model python script\n",
    "python3 load_model_to_device.py  --model_path ${MODELPATH} --device ${DEVICE}\n",
    "\n",
    "cd /output\n",
    "\n",
    "tar zcvf output.tgz stdout.log stderr.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_3g1pon8"
   },
   "source": [
    "## Step 3: Submitting a Job to Intel's DevCloud\n",
    "\n",
    "The code below will submit a job to an **IEI Tank-870** edge node with the following three devices:\n",
    "* **Intel Core i5 6500TE**\n",
    "* **Intel HD Graphics 530**\n",
    "* **Intel Neural Compute Stick 2**\n",
    "\n",
    "**Note**: We'll pass in a device type argument of `MULTI:MYRIAD,GPU,CPU` to load our model on all three devices at the same time. We'll need to use `FP16` as the model precision since we're loading our model on a GPU and VPU even though the recommended model precison is `FP32` for CPU.\n",
    "\n",
    "The `!qsub` command takes a few command line arguments:\n",
    "1. The first argument is the shell script filename - `load_multi_model_job.sh`. This should always be the first argument.\n",
    "2. The `-d` flag designates the directory where we want to run our job. We'll be running it in the current directory as denoted by `.`.\n",
    "3. The `-l` flag designates the node and quantity we want to request. The default quantity is 1, so the **1** after `nodes` is optional.\n",
    "\n",
    "4. The `-F` flag let's us pass in a string with all command line arguments we want to pass to our Python script.\n",
    "\n",
    "**Note**: There is an optional flag, `-N`, you may see in a few exercises. This is an argument that only works on Intel's DevCloud that allows you to name your job submission. This argument doesn't work in Udacity's workspace integration with Intel's DevCloud.\n",
    "\n",
    "In the cell below, we assign the returned value of the `!qsub` command to a variable `job_id_core`. This value is an array with a single string.\n",
    "\n",
    "Once the cell is run, this queues up a job on Intel's DevCloud and prints out the first value of this array below the cell, which is the job id.\n",
    "\n",
    "Click the **Submitting a Job to Intel's DevCloud** button below for a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_hox95hs"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_hox95hs-id_9pd5z9z\"><i></i><button>Submitting a Job to Intel's DevCloud</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "graffitiCellId": "id_e2hjrqo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GyfthgJawcONAnmLOs8vitJZ2nWwrhDF\n"
     ]
    }
   ],
   "source": [
    "job_id_core = !qsub load_multi_model_job.sh -d . -l nodes=1:tank-870:i5-6500te:intel-hd-530:intel-ncs2 -F \"MULTI:MYRIAD,GPU,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106\" -N store_core \n",
    "print(job_id_core[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_9wlti9i"
   },
   "source": [
    "## Step 4: Running liveQStat\n",
    "\n",
    "Running the `liveQStat` function, we can see the live status of our job. Running the this function will lock the cell and poll the job status 10 times. The cell is locked until this finishes polling 10 times or you can interrupt the kernel to stop it by pressing the stop button at the top: ![stop button](assets/interrupt_kernel.png)\n",
    "\n",
    "* `Q` status means our job is currently awaiting an available node\n",
    "* `R` status means our job is currently running on the requested node\n",
    "\n",
    "**Note**: In the demonstration, it is pointed out that `W` status means your job is done. This is no longer accurate. Once a job has finished running, it will no longer show in the list when running the `liveQStat` function.\n",
    "\n",
    "Click the **Running liveQStat** button below for a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_lnsl6m2"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_lnsl6m2-id_lauyzu5\"><i></i><button>Running LiveQStat</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "graffitiCellId": "id_k1sl02f"
   },
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_unlw6sn"
   },
   "source": [
    "## Step 5: Retrieving Output Files\n",
    "\n",
    "In this step, we'll be using the `getResults` function to retrieve our job's results. This function takes a few arguments.\n",
    "\n",
    "1. `job id` - This value is stored in the `job_id_core` variable we created during **Step 3**. Remember that this value is an array with a single string, so we access the string value using `job_id_core[0]`.\n",
    "2. `filename` - This value should match the filename of the compressed file we have in our `load_multi_model_job.sh` shell script. In this example, filename shoud be set to `output.tgz`.\n",
    "3. `blocking` - This is an optional argument and is set to `False` by default. If this is set to `True`, the cell is locked while waiting for the results to come back. There is a status indicator showing the cell is waiting on results.\n",
    "\n",
    "**Note**: The `getResults` function is unique to Udacity's workspace integration with Intel's DevCloud. When working on Intel's DevCloud environment, your job's results are automatically retrieved and placed in your working directory.\n",
    "\n",
    "Click the **Retrieving Output Files** button below for a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_v3k1sjd"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_v3k1sjd-id_emzwj1d\"><i></i><button>Retrieving Output Files</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "graffitiCellId": "id_ifn7fmp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:GyfthgJawcONAnmLOs8vitJZ2nWwrhDF) are ready.\n",
      "Please wait................................................Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(job_id_core[0], filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_5qqja6f"
   },
   "source": [
    "## Step 6: Viewing the Outputs\n",
    "In this step, we unpack the compressed file using `!tar zxf` and read the contents of the log files by using the `!cat` command.\n",
    "\n",
    "`stdout.log` should contain the printout of the print statement in our Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_n92ks6e"
   },
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_f44ura3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load model = 44.08395314216614 seconds\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_vuj3eg5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_to_device.py:13: DeprecationWarning: Reading network using constructor is deprecated. Please, use IECore.read_network() method instead\r\n",
      "  model=IENetwork(model_structure, model_weights)\r\n",
      "load_model_to_device.py:15: DeprecationWarning: IEPlugin class is deprecated. Please use IECore class instead.\r\n",
      "  plugin = IEPlugin(device=args.device)\r\n",
      "tar: stdout.log: file changed as we read it\r\n"
     ]
    }
   ],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_oh1ioro"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "graffiti": {
   "firstAuthorId": "dca260a8-2142-11ea-b0f7-6f7abbbf2f85",
   "id": "id_0qs9j6x",
   "language": "EN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
