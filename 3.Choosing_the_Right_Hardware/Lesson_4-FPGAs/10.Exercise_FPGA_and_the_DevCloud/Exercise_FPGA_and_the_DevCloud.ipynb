{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_2skrynt"
   },
   "source": [
    "# Exercise: FPGA and the DevCloud\n",
    "\n",
    "Now that we've walked through the process of requesting an edge node with a CPU and Intel® Arria 10 FPGA on Intel's DevCloud and loading a model on the Intel® Arria 10 FPGA, you will have the opportunity to do this yourself with the addition of running inference on an image.\n",
    "\n",
    "In this exercise, you will do the following:\n",
    "1. Write a Python script to load a model and run inference 10 times on a device on Intel's DevCloud.\n",
    "    * Calculate the time it takes to load the model.\n",
    "    * Calculate the time it takes to run inference 10 times.\n",
    "2. Write a shell script to submit a job to Intel's DevCloud.\n",
    "3. Submit a job using `qsub` on an **IEI Tank-870** edge node with an **Intel® Arria 10 FPGA**.\n",
    "4. Run `liveQStat` to view the status of your submitted jobs.\n",
    "5. Retrieve the results from your job.\n",
    "6. View the results.\n",
    "\n",
    "Click the **Exercise Overview** button below for a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_vskulnq"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_vskulnq-id_oudamc9\"><i></i><button>Exercise Overview</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_rxiw7xn"
   },
   "source": [
    "#### IMPORTANT: Set up paths so we can run Dev Cloud utilities\n",
    "You *must* run this every time you enter a Workspace session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "graffitiCellId": "id_j8qquf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n"
     ]
    }
   ],
   "source": [
    "%env PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_og4tvyg"
   },
   "source": [
    "## The Model\n",
    "\n",
    "We will be using the `vehicle-license-plate-detection-barrier-0106` model for this exercise. Remember that to run a model on the FPGA, we need to use `FP16` as the model precision.\n",
    "\n",
    "The model has already been downloaded for you in the `/data/models/intel` directory on Intel's DevCloud.\n",
    "\n",
    "We will be running inference on an image of a car. The path to the image is `/data/resources/car.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_xhmtvpp"
   },
   "source": [
    "# Step 1: Creating a Python Script\n",
    "\n",
    "The first step is to create a Python script that you can use to load the model and perform inference. We'll use the `%%writefile` magic to create a Python file called `inference_on_device.py`. In the next cell, you will need to complete the `TODO` items for this Python script.\n",
    "\n",
    "`TODO` items:\n",
    "\n",
    "1. Load the model\n",
    "\n",
    "2. Get the name of the input node\n",
    "\n",
    "3. Prepare the model for inference (create an input dictionary)\n",
    "\n",
    "4. Run inference 10 times in a loop\n",
    "\n",
    "If you get stuck, you can click on the **Show Solution** button below for a walkthrough with the solution code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "graffitiCellId": "id_4w6w5if"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_on_device.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_on_device.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IECore\n",
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    model=args.model_path\n",
    "    model_weights=model+'.bin'\n",
    "    model_structure=model+'.xml'\n",
    "    \n",
    "    start=time.time()\n",
    "    \n",
    "    # TODO: Load the model\n",
    "    model = IENetwork(model_structure, model_weights)\n",
    "    \n",
    "    core = IECore()\n",
    "    net = core.load_network(network = model, device_name = args.device, num_requests = 1)\n",
    "    \n",
    "    print(f\"Time taken to load model = {time.time()-start} seconds\")\n",
    "    \n",
    "    # Get the name of the input node\n",
    "    input_name=next(iter(model.inputs))\n",
    "    \n",
    "    # Reading and Preprocessing Image\n",
    "    input_img=cv2.imread('/data/resources/car.png')\n",
    "    input_img=cv2.resize(input_img, (300,300), interpolation = cv2.INTER_AREA)\n",
    "    input_img=np.moveaxis(input_img, -1, 0)\n",
    "\n",
    "    # TODO: Prepare the model for inference (create input dict etc.)\n",
    "    input_dict = {input_name:input_img}\n",
    "    \n",
    "    start=time.time()\n",
    "    for _ in range(10):\n",
    "        # TODO: Run Inference in a Loop\n",
    "        net.infer(input_dict)\n",
    "    \n",
    "    print(f\"Time Taken to run 10 Inference on FPGA is = {time.time()-start} seconds\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', required=True)\n",
    "    parser.add_argument('--device', default=None)\n",
    "    \n",
    "    args=parser.parse_args() \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_f28ff2h"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_f28ff2h-id_4psdryf\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_jwx8ifc"
   },
   "source": [
    "## Step 2: Creating a Job Submission Script\n",
    "\n",
    "To submit a job to the DevCloud, you'll need to create a shell script. Similar to the Python script above, we'll use the `%%writefile` magic command to create a shell script called `inference_fpga_model_job.sh`. In the next cell, you will need to complete the `TODO` items for this shell script.\n",
    "\n",
    "`TODO` items:\n",
    "1. Create three variables:\n",
    "    * `DEVICE` - Assign the value as the first argument passed into the shell script.\n",
    "    * `MODELPATH` - Assign the value as the second argument passed into the shell script.\n",
    "2. Call the Python script using the three variable values as the command line argument\n",
    "\n",
    "If you get stuck, you can click on the **Show Solution** button below for a walkthrough with the solution code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "graffitiCellId": "id_hmaeu94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_fpga_model_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_fpga_model_job.sh\n",
    "#!/bin/bash\n",
    "\n",
    "exec 1>/output/stdout.log 2>/output/stderr.log\n",
    "\n",
    "mkdir -p /output\n",
    "\n",
    "# TODO: Create DEVICE variable\n",
    "DEVICE=$1\n",
    "# TODO: Create MODELPATH variable\n",
    "MODELPATH=$2\n",
    "\n",
    "export AOCL_BOARD_PACKAGE_ROOT=/opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/BSP/a10_1150_sg2\n",
    "\n",
    "source /opt/altera/aocl-pro-rte/aclrte-linux64/init_opencl.sh\n",
    "aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/2020-2_PL2_FP16_MobileNet_Clamp.aocx\n",
    "\n",
    "export CL_CONTEXT_COMPILER_MODE_INTELFPGA=3\n",
    "\n",
    "# TODO: Call the Python script\n",
    "python3 inference_on_device.py --model_path ${MODELPATH} --device ${DEVICE}\n",
    "\n",
    "cd /output\n",
    "\n",
    "tar zcvf output.tgz * # compresses all files in the current directory (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_5e0vxvt"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_5e0vxvt-id_5zk2mzh\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_3w1m2ga"
   },
   "source": [
    "## Step 3: Submitting a Job to Intel's DevCloud\n",
    "\n",
    "In the next cell, you will write your `!qsub` command to load your model and run inference on the **IEI Tank-870** edge node with an **Intel Core i5** CPU and an **Intel® Arria 10 FPGA**.\n",
    "\n",
    "Your `!qsub` command should take the following flags and arguments:\n",
    "1. The first argument should be the shell script filename\n",
    "2. `-d` flag - This argument should be `.`\n",
    "3. `-l` flag - This argument should request an edge node with an **IEI Tank-870**. The default quantity is 1, so the **1** after `nodes` is optional. \n",
    "    * **Intel Core i5 6500TE** for your `CPU`.\n",
    "    * **Intel® Arria 10** for your `FPGA`.\n",
    "\n",
    "To get the queue labels for these devices, you can go to [this link](https://devcloud.intel.com/edge/get_started/devcloud/)\n",
    "\n",
    "4. `-F` flag - This argument should contain the two values to assign to the variables of the shell script:\n",
    "    * **DEVICE** - Device type for the job: `FPGA`. Remember that we need to use the **Heterogenous plugin** (HETERO) to run inference on the FPGA.\n",
    "    * **MODELPATH** - Full path to the model for the job. As a reminder, the model is located in `/data/models/intel`.\n",
    "\n",
    "**Note**: There is an optional flag, `-N`, you may see in a few exercises. This is an argument that only works on Intel's DevCloud that allows you to name your job submission. This argument doesn't work in Udacity's workspace integration with Intel's DevCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "graffitiCellId": "id_0okzrke"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JycMcbQY2A3tsHOQKV7CXWjX8e7UEJew\n"
     ]
    }
   ],
   "source": [
    "job_id_core = !qsub inference_fpga_model_job.sh -d . -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"HETERO:FPGA,CPU /data/models/intel/vehicle-license-plate-detection-barrier-0106/FP16/vehicle-license-plate-detection-barrier-0106\" -N store_core # TODO: Write qsub command\n",
    "print(job_id_core[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_yr40vov"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_yr40vov-id_cvo0xg6\"><i></i><button>Show Solution</button></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_508yfca"
   },
   "source": [
    "## Step 4: Running liveQStat\n",
    "\n",
    "Running the `liveQStat` function, we can see the live status of our job. Running the this function will lock the cell and poll the job status 10 times. The cell is locked until this finishes polling 10 times or you can interrupt the kernel to stop it by pressing the stop button at the top: ![stop button](assets/interrupt_kernel.png)\n",
    "\n",
    "* `Q` status means our job is currently awaiting an available node\n",
    "* `R` status means our job is currently running on the requested node\n",
    "\n",
    "**Note**: In the demonstration, it is pointed out that `W` status means your job is done. This is no longer accurate. Once a job has finished running, it will no longer show in the list when running the `liveQStat` function.\n",
    "\n",
    "Click the **Running liveQStat** button below for a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ecvm8yr"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_ecvm8yr-id_nnpaoep\"><i></i><button>Running liveQStat</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "graffitiCellId": "id_q0j984n"
   },
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_dq0gcof"
   },
   "source": [
    "## Step 5: Retrieving Output Files\n",
    "\n",
    "In this step, we'll be using the `getResults` function to retrieve our job's results. This function takes a few arguments.\n",
    "\n",
    "1. `job id` - This value is stored in the `job_id_core` variable we created during **Step 3**. Remember that this value is an array with a single string, so we access the string value using `job_id_core[0]`.\n",
    "2. `filename` - This value should match the filename of the compressed file we have in our `inference_fpga_model_job.sh` shell script.\n",
    "3. `blocking` - This is an optional argument and is set to `False` by default. If this is set to `True`, the cell is locked while waiting for the results to come back. There is a status indicator showing the cell is waiting on results.\n",
    "\n",
    "**Note**: The `getResults` function is unique to Udacity's workspace integration with Intel's DevCloud. When working on Intel's DevCloud environment, your job's results are automatically retrieved and placed in your working directory.\n",
    "\n",
    "Click the **Retrieving Output Files** button below for a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_s7wimuv"
   },
   "source": [
    "<span class=\"graffiti-highlight graffiti-id_s7wimuv-id_xm8qs9p\"><i></i><button>Retrieving Output Files</button></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "graffitiCellId": "id_b1elza3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:JycMcbQY2A3tsHOQKV7CXWjX8e7UEJew) are ready.\n",
      "Please wait...Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "\n",
    "get_results.getResults(job_id_core[0], filename=\"output.tgz\", blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_ldxhc7s"
   },
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_ee5qrbj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTELFPGAOCLSDKROOT is not set\r\n",
      "Using script's current directory (/opt/altera/aocl-pro-rte/aclrte-linux64)\r\n",
      "\r\n",
      "aoc was not found, but aocl was found. Assuming only RTE is installed.\r\n",
      "\r\n",
      "AOCL_BOARD_PACKAGE_ROOT is set to /opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/BSP/a10_1150_sg2. Using that.\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/bin to PATH\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/linux64/lib to LD_LIBRARY_PATH\r\n",
      "Adding /opt/altera/aocl-pro-rte/aclrte-linux64/host/linux64/lib to LD_LIBRARY_PATH\r\n",
      "Adding /opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/BSP/a10_1150_sg2/linux64/lib to LD_LIBRARY_PATH\r\n",
      "aocl program: Running program from /opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/BSP/a10_1150_sg2/linux64/libexec\r\n",
      "Programming device: a10gx_2ddr : Intel Vision Accelerator Design with Intel Arria 10 FPGA (acla10_1150_sg20)\r\n",
      "Program succeed. \r\n",
      "Time taken to load model = 3.7116880416870117 seconds\r\n",
      "Time Taken to run 10 Inference on FPGA is = 0.07988500595092773 seconds\r\n",
      "stderr.log\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_8316jkh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_on_device.py:17: DeprecationWarning: Reading network using constructor is deprecated. Please, use IECore.read_network() method instead\r\n",
      "  model = IENetwork(model_structure, model_weights)\r\n"
     ]
    }
   ],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "graffitiCellId": "id_so5c0dh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "graffiti": {
   "firstAuthorId": "10505542082",
   "id": "id_am6qdje",
   "language": "EN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
