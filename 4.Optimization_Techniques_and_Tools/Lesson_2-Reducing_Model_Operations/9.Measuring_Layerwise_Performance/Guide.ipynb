{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "ulab": {
     "buttons": {
      "ulab-button-1f07fc1c": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "RIGHT CLICK ME",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      },
      "ulab-button-276ae57d": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "Source OpenVINO Environment",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Measuring Layerwise Performance\n",
    "\n",
    "So far, you have seen the effect that using efficient layers has on the overall inference time of the model. Since the networks we have been working on so far were small, it was easy for us to calculate the FLOPs for each layer and seewhich layers were the bottleneck in our model.\n",
    "\n",
    "However, in case of larger models, calculating the FLOPs for each model quickly becomes very tedious and difficult. Moreover, just measuring the inference time of the model does not give us much information about which layer might be taking more time to compute in our model.\n",
    "\n",
    "In this concept, we will use the `get_perf_counts` API available in OpenVINO to get the performance of each layer in our model. By identifying the bottlenecks in our models performance, we can then remove, or change those layers to make our model inference time faster.\n",
    "\n",
    "Follow along with the video and try to run the code\n",
    "\n",
    "Before running the code, make sure you source the OpenVINO environment by clicking the button below:\n",
    "\n",
    "<button id=\"ulab-button-276ae57d\" class=\"ulab-btn--primary\"></button>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Quiz\n",
    "Which type of layer took the most time to execute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-c4574c14": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "Fully Connected Layer"
      }
     }
    }
   },
   "source": [
    "**Correct!**\n",
    "\n",
    "Since the flattened output from the previous convolutional layer is quite large, the total FLOPs in this layer is much more than the convolutional layers. This is why it took more time to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-a8ec2fc5": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "Convolutional Layer"
      }
     }
    }
   },
   "source": [
    "Remember that execution time depends on the total FLOPs that are needed to execute that layer. While convolutional layers are more complex, the first fully connected layer actually requires more FLOPs to execute than the convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "video": {
     "duration": 223,
     "id": "105689",
     "subtitles": [],
     "topher_id": "3105e173-67a5-11ea-8bf3-0242ac110002",
     "transcodings": {
      "uri_480p_1000kbps_mp4": "https://video.udacity-data.com/topher/2020/March/5e6fac91_nd131-c03-l01-a08-measuring-layerwise-performance/nd131-c03-l01-a08-measuring-layerwise-performance_480p_1000kbps.mp4",
      "uri_480p_mp4": "https://video.udacity-data.com/topher/2020/March/5e6fac91_nd131-c03-l01-a08-measuring-layerwise-performance/nd131-c03-l01-a08-measuring-layerwise-performance_480p.mp4",
      "uri_720p_mp4": "https://video.udacity-data.com/topher/2020/March/5e6fac91_nd131-c03-l01-a08-measuring-layerwise-performance/nd131-c03-l01-a08-measuring-layerwise-performance_720p.mp4",
      "uri_hls": "https://video.udacity-data.com/topher/2020/March/5e6fac91_nd131-c03-l01-a08-measuring-layerwise-performance/hls/playlist.m3u8"
     },
     "youtube_id": "5P5MXhJaRS8"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "ulab_nb_type": "guided"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
