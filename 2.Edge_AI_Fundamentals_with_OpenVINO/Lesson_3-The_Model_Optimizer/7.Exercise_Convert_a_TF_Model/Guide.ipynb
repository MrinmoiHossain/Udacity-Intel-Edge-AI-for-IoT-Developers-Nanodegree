{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-3e515cac": {
       "bashCommand": "pip install requests pyyaml -t /usr/local/lib/python3.5/dist-packages && clear && source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Loading Pre-Trained Models\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-3e515cac\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "In this exercise, you'll work to download and load a few of the pre-trained models available \n",
    "in the OpenVINO toolkit.\n",
    "\n",
    "First, you can navigate to the [Pre-Trained Models list](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models) in a separate window or tab, as well as the page that gives all of the model names [here](https://docs.openvinotoolkit.org/latest/_models_intel_index.html).\n",
    "\n",
    "Your task here is to download the below three pre-trained models using the Model Downloader tool, as detailed on the same page as the different model names. Note that you *do not need to download all of the available pre-trained models* - doing so would cause your workspace to crash, as the workspace will limit you to 3 GB of downloaded models.\n",
    "\n",
    "### Task 1 - Find the Right Models\n",
    "Using the [Pre-Trained Model list](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models), determine which models could accomplish the following tasks (there may be some room here in determining which model to download):\n",
    "- Human Pose Estimation\n",
    "- Text Detection\n",
    "- Determining Car Type & Color\n",
    "\n",
    "### Task 2 - Download the Models\n",
    "Once you have determined which model best relates to the above tasks, use the Model Downloader tool to download them into the workspace for the following precision levels:\n",
    "- Human Pose Estimation: All precision levels\n",
    "- Text Detection: FP16 only\n",
    "- Determining Car Type & Color: INT8 only\n",
    "\n",
    "**Note**: When downloading the models in the workspace, add the `-o` argument (along with any other necessary arguments) with `/home/workspace` as the output directory. The default download directory will not allow the files to be written there within the workspace, as it is a read-only directory.\n",
    "\n",
    "### Task 3 - Verify the Downloads\n",
    "You can verify the download of these models by navigating to: `/home/workspace/intel` (if you followed the above note), and checking whether a directory was created for each of the three models, with included subdirectories for each precision, with respective `.bin` and `.xml` for each model.\n",
    "\n",
    "**Hint**: Use the `-h` command with the Model Downloader tool if you need to check out the possible arguments to include when downloading specific models and precisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-dcdc9e86": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Preprocessing Inputs\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-dcdc9e86\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "Now that we have a few pre-trained models downloaded, it's time to preprocess the inputs\n",
    "to match what each of the models expects as their input. We'll use the same models as before\n",
    "as a basis for determining the preprocessing necessary for each input file.\n",
    "\n",
    "As a reminder, our three models are:\n",
    "- Human Pose Estimation: [human-pose-estimation-0001](https://docs.openvinotoolkit.org/latest/_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html)\n",
    "- Text Detection: [text-detection-0004](http://docs.openvinotoolkit.org/latest/_models_intel_text_detection_0004_description_text_detection_0004.html)\n",
    "- Determining Car Type & Color: [vehicle-attributes-recognition-barrier-0039](https://docs.openvinotoolkit.org/latest/_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html)\n",
    "\n",
    "**Note:** For ease of use, these models have been added into the `/home/workspace/models`\n",
    "directory. For example, if you need to use the Text Detection model, you could find it at:\n",
    "\n",
    "```bash\n",
    "/home/workspace/models/text_detection_0004.xml\n",
    "```\n",
    "\n",
    "Each link above contains the documentation for the related model. In our case, we want to \n",
    "focus on the **Inputs** section of the page, wherein important information regarding the input\n",
    "shape, order of the shape (such as color channel first or last), and the order of the color\n",
    "channels, is included.\n",
    "\n",
    "Your task is to fill out the code in three functions within `preprocess_inputs.py`, one for \n",
    "each of the three models. We have also included a potential sample image for each of the \n",
    "three models, that will be used with `test.py` to check whether the\n",
    "input for each model has been adjusted as expected for proper model input.\n",
    "\n",
    "Note that each image is **currently loaded as BGR with H, W, C order** in the `test.py` file,\n",
    "so any necessary preprocessing to change that should occur in your three work files. \n",
    "Note that **BGR** order is used, as the OpenCV function we use to read images loads as\n",
    "BGR, and not RGB.\n",
    "\n",
    "When finished, you should be able to run the `test.py` file and pass all three tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-60888dc0": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Deploy Your First Edge App\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-60888dc0\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "So far, you've downloaded some pre-trained models, handled their inputs, and learned how\n",
    "to handle outputs. In this exercise, you'll implement the handling of the outputs of our three\n",
    "models from before, and get to see inference actually performed by adding these models\n",
    "to some example edge applications. \n",
    "\n",
    "There's a lot of code still involved behind the scenes here. With the Pre-Trained Models \n",
    "available with the OpenVINO toolkit, you don't need to worry about the Model Optimizer, but\n",
    "there is still work done to load the model into the Inference Engine. We won't learn about \n",
    "this code until later, so in this case, you'll just need to call your functions to handle the input\n",
    "and output of the model within the app.\n",
    "\n",
    "If you do want a sneak preview of some of the code that interfaces with the Inference Engine,\n",
    "you can check it out in `inference.py`. You'll work out of the `handle_models.py` file, as \n",
    "well as adding functions calls within the edge app in `app.py`.\n",
    "\n",
    "## TODOs\n",
    "\n",
    "In `handle_models.py`, you will need to implement `handle_pose`, `handle_text`, and\n",
    "`handle_car`.\n",
    "\n",
    "In `app.py`, first, you'll need to use the input shape of the network to call the `preprocessing`\n",
    "function. Then, you need to call `handle_output` with the appropriate model argument \n",
    "in order to get the right handling function. With that function, you can then feed the output\n",
    "of the inference request in in order to extract the output. \n",
    "\n",
    "Note that there is some additional post-processing done for you in `create_output_image`\n",
    "within `app.py` to help display the output back onto the input image.\n",
    "\n",
    "## Testing the apps\n",
    "\n",
    "To test your implementations, you can use `app.py` to run each edge application, with\n",
    "the following arguments:\n",
    "- `-t`: The model type,  which should be one of `\"POSE\"`, `\"TEXT\"`, or `\"CAR_META\"`\n",
    "- `-m`: The location of the model .xml file\n",
    "- `-i`: The location of the input image used for testing\n",
    "- `-c`: A CPU extension file, if applicable. See below for what this is for the workspace.\n",
    "The results of your output will be saved down for viewing in the `outputs` directory.\n",
    "\n",
    "As an example, here is an example of running the app with related arguments:\n",
    "\n",
    "```\n",
    "python app.py -i \"images/blue-car.jpg\" -t \"CAR_META\" -m \"/home/workspace/models/vehicle-attributes-recognition-barrier-0039.xml\" -c \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "```\n",
    "\n",
    "## Model Documentation\n",
    "\n",
    "Once again, here are the links to the models, so you can use the **Output** section to help\n",
    "you get started (there are additional comments in the code to assist):\n",
    "\n",
    "- Human Pose Estimation: [human-pose-estimation-0001](https://docs.openvinotoolkit.org/latest/_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html)\n",
    "- Text Detection: [text-detection-0004](http://docs.openvinotoolkit.org/latest/_models_intel_text_detection_0004_description_text_detection_0004.html)\n",
    "- Determining Car Type & Color: [vehicle-attributes-recognition-barrier-0039](https://docs.openvinotoolkit.org/latest/_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-663e2c8b": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Convert a TensorFlow Model\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-663e2c8b\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "In this exercise, you'll convert a TensorFlow Model from the Object Detection Model Zoo\n",
    "into an Intermediate Representation using the Model Optimizer.\n",
    "\n",
    "As noted in the related [documentation](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html), \n",
    "there is a difference in method when using a frozen graph vs. an unfrozen graph. Since\n",
    "freezing a graph is a TensorFlow-based function and not one specific to OpenVINO itself,\n",
    "in this exercise, you will only need to work with a frozen graph. However, I encourage you to\n",
    "try to freeze and load an unfrozen model on your own as well.\n",
    "\n",
    "For this exercise, first download the SSD MobileNet V2 COCO model from [here](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz). Use the `tar -xvf` \n",
    "command with the downloaded file to unpack it.\n",
    "\n",
    "From there, find the **Convert a TensorFlow\\* Model** header in the documentation, and\n",
    "feed in the downloaded SSD MobileNet V2 COCO model's `.pb` file. \n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "**Note**: Converting the TF model will take a little over one minute in the workspace.\n",
    "\n",
    "### Hints & Troubleshooting\n",
    "\n",
    "Make sure to pay attention to the note in this section regarding the \n",
    "`--reverse_input_channels` argument. \n",
    "If you are unsure about this argument, you can read more [here](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model_General.html#when_to_reverse_input_channels).\n",
    "\n",
    "There is additional documentation specific to converting models from TensorFlow's Object\n",
    "Detection Zoo [here](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html).\n",
    "You will likely need both the `--tensorflow_use_custom_operations_config` and\n",
    "`--tensorflow_object_detection_api_pipeline_config` arguments fed with their \n",
    "related files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-d0a57724": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Convert a Caffe Model\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-d0a57724\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "In this exercise, you'll convert a Caffe Model into an Intermediate Representation using the \n",
    "Model Optimizer. You can find the related documentation [here](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html).\n",
    "\n",
    "For this exercise, first download the SqueezeNet V1.1 model by cloning [this repository](https://github.com/DeepScale/SqueezeNet). \n",
    "\n",
    "Follow the documentation above and feed in the Caffe model to the Model Optimizer.\n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "### Hints & Troubleshooting\n",
    "\n",
    "You will need to specify `--input_proto` if the `.prototxt` file is not named the same as the model.\n",
    "\n",
    "There is an important note in the documentation after the section **Supported Topologies** \n",
    "regarding Caffe models trained on ImageNet. If you notice poor performance in inference, you\n",
    "may need to specify mean and scale values in your arguments.\n",
    "\n",
    "```\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-0bd71d51": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Convert an ONNX Model\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-0bd71d51\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "### Exercise Instructions\n",
    "\n",
    "In this exercise, you'll convert an ONNX Model into an Intermediate Representation using the \n",
    "Model Optimizer. You can find the related documentation [here](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html).\n",
    "\n",
    "For this exercise, first download the bvlc_alexnet model from [here](https://s3.amazonaws.com/download.onnx/models/opset_8/bvlc_alexnet.tar.gz). Use the `tar -xvf` command with the downloaded file to unpack it.\n",
    "\n",
    "Follow the documentation above and feed in the ONNX model to the Model Optimizer.\n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "### PyTorch models\n",
    "\n",
    "Note that we will only cover converting directly from an ONNX model here. If you are interested\n",
    "in converting a PyTorch model using ONNX for use with OpenVINO, check out this [link](https://michhar.github.io/convert-pytorch-onnx/) for the steps to do so. From there, you can follow the steps in the rest\n",
    "of this exercise once you have an ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-c7cfa177": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Custom Layers\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-c7cfa177\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "This exercise is adapted from [this repository](https://github.com/david-drew/OpenVINO-Custom-Layers).\n",
    "\n",
    "Note that the classroom workspace is running OpenVINO 2019.r3, while this exercise was\n",
    "originally created for 2019.r2. This exercise will work appropriately in the workspace, but there\n",
    "may be some other differences you need to account for if you use a custom layer yourself.\n",
    "\n",
    "The below steps will walk you through the full walkthrough of creating a custom layer; as such,\n",
    "there is not a related solution video. Note that custom layers is an advanced topic, and one\n",
    "that is not expected to be used often (if at all) in most use cases of the OpenVINO toolkit. This\n",
    "exercise is meant to introduce you to the concept, but you won't need to use it again in the \n",
    "rest of this course.\n",
    "\n",
    "## Example Custom Layer: The Hyperbolic Cosine (cosh) Function\n",
    "\n",
    "We will follow the steps involved for implementing a custom layer using the simple \n",
    "hyperbolic cosine (cosh) function. The cosh function is mathematically calculated as:\n",
    "\n",
    "```\n",
    "cosh(x) = (e^x + e^-x) / 2\n",
    "```\n",
    "\n",
    "As a function that calculates a value for the given value x, the cosh function is very simple \n",
    "when compared to most custom layers. Though the cosh function may not represent a \"real\" \n",
    "custom layer, it serves the purpose of this tutorial as an example for working through the steps \n",
    "for implementing a custom layer.\n",
    "\n",
    "Move to the next page to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "First, export the below paths to shorten some of what you need to enter later:\n",
    "\n",
    "```\n",
    "export CLWS=/home/workspace/cl_tutorial\n",
    "export CLT=$CLWS/OpenVINO-Custom-Layers\n",
    "```\n",
    "\n",
    "Then run the following to create the TensorFlow model including the `cosh` layer.\n",
    "\n",
    "```\n",
    "mkdir $CLWS/tf_model\n",
    "python $CLT/create_tf_model/build_cosh_model.py $CLWS/tf_model\n",
    "```\n",
    "\n",
    "You should receive a message similar to:\n",
    "\n",
    "```\n",
    "Model saved in path: /tf_model/model.ckpt\n",
    "```\n",
    "\n",
    "## Creating the *`cosh`* Custom Layer\n",
    "\n",
    "### Generate the Extension Template Files Using the Model Extension Generator\n",
    "\n",
    "We will use the Model Extension Generator tool to automatically create templates for all the \n",
    "extensions needed by the Model Optimizer to convert and the Inference Engine to execute \n",
    "the custom layer.  The extension template files will be partially replaced by Python and C++ \n",
    "code to implement the functionality of `cosh` as needed by the different tools.  To create \n",
    "the four extensions for the `cosh` custom layer, we run the Model Extension Generator \n",
    "with the following options:\n",
    "\n",
    "- `--mo-tf-ext` = Generate a template for a Model Optimizer TensorFlow extractor\n",
    "- `--mo-op` = Generate a template for a Model Optimizer custom layer operation\n",
    "- `--ie-cpu-ext` = Generate a template for an Inference Engine CPU extension\n",
    "- `--ie-gpu-ext` = Generate a template for an Inference Engine GPU extension \n",
    "- `--output_dir` = set the output directory.  Here we are using `$CLWS/cl_cosh` as the target directory to store the output from the Model Extension Generator.\n",
    "\n",
    "To create the four extension templates for the `cosh` custom layer, given we are in the `$CLWS`\n",
    "directory, we run the command:\n",
    "\n",
    "```\n",
    "mkdir cl_cosh\n",
    "```\n",
    "\n",
    "```bash\n",
    "python /opt/intel/openvino/deployment_tools/tools/extension_generator/extgen.py new --mo-tf-ext --mo-op --ie-cpu-ext --ie-gpu-ext --output_dir=$CLWS/cl_cosh\n",
    "```\n",
    "\n",
    "The Model Extension Generator will start in interactive mode and prompt us with questions \n",
    "about the custom layer to be generated.  Use the text between the `[]`'s to answer each \n",
    "of the Model Extension Generator questions as follows:\n",
    "\n",
    "```\n",
    "Enter layer name: \n",
    "[cosh]\n",
    "\n",
    "Do you want to automatically parse all parameters from the model file? (y/n)\n",
    "...\n",
    "[n]\n",
    "\n",
    "Enter all parameters in the following format:\n",
    "...\n",
    "Enter 'q' when finished:\n",
    "[q]\n",
    "\n",
    "Do you want to change any answer (y/n) ? Default 'no'\n",
    "[n]\n",
    "\n",
    "Do you want to use the layer name as the operation name? (y/n)\n",
    "[y]\n",
    "\n",
    "Does your operation change shape? (y/n)  \n",
    "[n]\n",
    "\n",
    "Do you want to change any answer (y/n) ? Default 'no'\n",
    "[n]\n",
    "```\n",
    "\n",
    "When complete, the output text will appear similar to:\n",
    "```\n",
    "Stub file for TensorFlow Model Optimizer extractor is in /home/<user>/cl_tutorial/cl_cosh/user_mo_extensions/front/tf folder\n",
    "Stub file for the Model Optimizer operation is in /home/<user>/cl_tutorial/cl_cosh/user_mo_extensions/ops folder\n",
    "Stub files for the Inference Engine CPU extension are in /home/<user>/cl_tutorial/cl_cosh/user_ie_extensions/cpu folder\n",
    "Stub files for the Inference Engine GPU extension are in /home/<user>/cl_tutorial/cl_cosh/user_ie_extensions/gpu folder\n",
    "```\n",
    "\n",
    "Template files (containing source code stubs) that may need to be edited have just been \n",
    "created in the following locations:\n",
    "\n",
    "- TensorFlow Model Optimizer extractor extension: \n",
    "  - `$CLWS/cl_cosh/user_mo_extensions/front/tf/`\n",
    "  - `cosh_ext.py`\n",
    "- Model Optimizer operation extension:\n",
    "  - `$CLWS/cl_cosh/user_mo_extensions/ops`\n",
    "  - `cosh.py`\n",
    "- Inference Engine CPU extension:\n",
    "  - `$CLWS/cl_cosh/user_ie_extensions/cpu`\n",
    "  - `ext_cosh.cpp`\n",
    "  - `CMakeLists.txt`\n",
    "- Inference Engine GPU extension:\n",
    "  - `$CLWS/cl_cosh/user_ie_extensions/gpu`\n",
    "  - `cosh_kernel.cl`\n",
    "  - `cosh_kernel.xml`\n",
    "\n",
    "Instructions on editing the template files are provided in later parts of this tutorial.  \n",
    "For reference, or to copy to make the changes quicker, pre-edited template files are provided \n",
    "by the tutorial in the `$CLT` directory.\n",
    "\n",
    "Move to the next page to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Using Model Optimizer to Generate IR Files Containing the Custom Layer \n",
    "\n",
    "We will now use the generated extractor and operation extensions with the Model Optimizer \n",
    "to generate the model IR files needed by the Inference Engine.  The steps covered are:\n",
    "\n",
    "1. Edit the extractor extension template file (already done - we will review it here)\n",
    "2. Edit the operation extension template file (already done - we will review it here)\n",
    "3. Generate the Model IR Files\n",
    "\n",
    "### Edit the Extractor Extension Template File\n",
    "\n",
    "For the `cosh` custom layer, the generated extractor extension does not need to be modified \n",
    "because the layer parameters are used without modification.  Below is a walkthrough of \n",
    "the Python code for the extractor extension that appears in the file \n",
    "`$CLWS/cl_cosh/user_mo_extensions/front/tf/cosh_ext.py`.\n",
    "1. Using the text editor, open the extractor extension source file `$CLWS/cl_cosh/user_mo_extensions/front/tf/cosh_ext.py`.\n",
    "2. The class is defined with the unique name `coshFrontExtractor` that inherits from the base extractor `FrontExtractorOp` class.  The class variable `op` is set to the name of the layer operation and `enabled` is set to tell the Model Optimizer to use (`True`) or exclude (`False`) the layer during processing.\n",
    "\n",
    "    ```python\n",
    "    class coshFrontExtractor(FrontExtractorOp):\n",
    "        op = 'cosh' \n",
    "        enabled = True\n",
    "    ```\n",
    "\n",
    "3. The `extract` function is overridden to allow modifications while extracting parameters from layers within the input model.\n",
    "\n",
    "    ```python\n",
    "    @staticmethod\n",
    "    def extract(node):\n",
    "    ```\n",
    "\n",
    "4. The layer parameters are extracted from the input model and stored in `param`.  This is where the layer parameters in `param` may be retrieved and used as needed.  For the `cosh` custom layer, the `op` attribute is simply set to the name of the operation extension used.\n",
    "\n",
    "    ```python\n",
    "    proto_layer = node.pb\n",
    "    param = proto_layer.attr\n",
    "    # extracting parameters from TensorFlow layer and prepare them for IR\n",
    "    attrs = {\n",
    "        'op': __class__.op\n",
    "    }\n",
    "    ```\n",
    "\n",
    "5. The attributes for the specific node are updated. This is where we can modify or create attributes in `attrs` before updating `node` with the results and the `enabled` class variable is returned.\n",
    "\n",
    "    ```python\n",
    "    # update the attributes of the node\n",
    "    Op.get_op_class_by_name(__class__.op).update_node_stat(node, attrs)\n",
    "    \n",
    "    return __class__.enabled\n",
    "    ```\n",
    "\n",
    "### Edit the Operation Extension Template File\n",
    "\n",
    "For the `cosh` custom layer, the generated operation extension does not need to be modified \n",
    "because the shape (i.e., dimensions) of the layer output is the same as the input shape.  \n",
    "Below is a walkthrough of the Python code for the operation extension that appears in \n",
    "the file  `$CLWS/cl_cosh/user_mo_extensions/ops/cosh.py`.\n",
    "\n",
    "1. Using the text editor, open the operation extension source file `$CLWS/cl_cosh/user_mo_extensions/ops/cosh.py` \n",
    "2. The class is defined with the unique name `coshOp` that inherits from the base operation `Op` class.  The class variable `op` is set to `'cosh'`, the name of the layer operation.\n",
    "\n",
    "    ```python\n",
    "    class coshOp(Op):\n",
    "    op = 'cosh'\n",
    "    ```\n",
    "\n",
    "3. The `coshOp` class initializer `__init__` function will be called for each layer created.  The initializer must initialize the super class `Op` by passing the `graph` and `attrs` arguments along with a dictionary of the mandatory properties for the `cosh` operation layer that define the type (`type`), operation (`op`), and inference function (`infer`).  This is where any other initialization needed by the `coshOP` operation can be specified.\n",
    "\n",
    "    ```python\n",
    "    def __init__(self, graph, attrs):\n",
    "        mandatory_props = dict(\n",
    "            type=__class__.op,\n",
    "            op=__class__.op,\n",
    "            infer=coshOp.infer            \n",
    "        )\n",
    "    super().__init__(graph, mandatory_props, attrs)\n",
    "    ```\n",
    "\n",
    "4. The `infer` function is defined to provide the Model Optimizer information on a layer, specifically returning the shape of the layer output for each node.  Here, the layer output shape is the same as the input and the value of the helper function `copy_shape_infer(node)` is returned.\n",
    "\n",
    "    ```python\n",
    "    @staticmethod\n",
    "    def infer(node: Node):\n",
    "        # ==========================================================\n",
    "        # You should add your shape calculation implementation here\n",
    "        # If a layer input shape is different to the output one\n",
    "        # it means that it changes shape and you need to implement\n",
    "        # it on your own. Otherwise, use copy_shape_infer(node).\n",
    "        # ==========================================================\n",
    "        return copy_shape_infer(node)\n",
    "    ```\n",
    "\n",
    "### Generate the Model IR Files\n",
    "\n",
    "With the extensions now complete, we use the Model Optimizer to convert and optimize \n",
    "the example TensorFlow model into IR files that will run inference using the Inference Engine.  \n",
    "To create the IR files, we run the Model Optimizer for TensorFlow `mo_tf.py` with \n",
    "the following options:\n",
    "\n",
    "- `--input_meta_graph model.ckpt.meta`\n",
    "  - Specifies the model input file.  \n",
    "\n",
    "- `--batch 1`\n",
    "  - Explicitly sets the batch size to 1 because the example model has an input dimension of \"-1\".\n",
    "  - TensorFlow allows \"-1\" as a variable indicating \"to be filled in later\", however the Model Optimizer requires explicit information for the optimization process.  \n",
    "\n",
    "- `--output \"ModCosh/Activation_8/softmax_output\"`\n",
    "  - The full name of the final output layer of the model.\n",
    "\n",
    "- `--extensions $CLWS/cl_cosh/user_mo_extensions`\n",
    "  - Location of the extractor and operation extensions for the custom layer to be used by the Model Optimizer during model extraction and optimization. \n",
    "\n",
    "- `--output_dir $CLWS/cl_ext_cosh`\n",
    "  - Location to write the output IR files.\n",
    "\n",
    "To create the model IR files that will include the `cosh` custom layer, we run the commands:\n",
    "\n",
    "```bash\n",
    "cd $CLWS/tf_model\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_meta_graph model.ckpt.meta --batch 1 --output \"ModCosh/Activation_8/softmax_output\" --extensions $CLWS/cl_cosh/user_mo_extensions --output_dir $CLWS/cl_ext_cosh\n",
    "```\n",
    "\n",
    "The output will appear similar to:\n",
    "\n",
    "```\n",
    "[ SUCCESS ] Generated IR model.\n",
    "[ SUCCESS ] XML file: /home/<user>/cl_tutorial/cl_ext_cosh/model.ckpt.xml\n",
    "[ SUCCESS ] BIN file: /home/<user>/cl_tutorial/cl_ext_cosh/model.ckpt.bin\n",
    "[ SUCCESS ] Total execution time: x.xx seconds.\n",
    "```\n",
    "\n",
    "Move to the next page to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Inference Engine Custom Layer Implementation for the Intel® CPU\n",
    "\n",
    "We will now use the generated CPU extension with the Inference Engine to execute \n",
    "the custom layer on the CPU.  The steps are:\n",
    "\n",
    "1. Edit the CPU extension template files.\n",
    "2. Compile the CPU extension library.\n",
    "3. Execute the Model with the custom layer.\n",
    "\n",
    "You *will* need to make the changes in this section to the related files.\n",
    "\n",
    "Note that the classroom workspace only has an Intel CPU available, so we will not perform\n",
    "the necessary steps for GPU usage with the Inference Engine.\n",
    "\n",
    "### Edit the CPU Extension Template Files\n",
    "\n",
    "The generated CPU extension includes the template file `ext_cosh.cpp` that must be edited \n",
    "to fill-in the functionality of the `cosh` custom layer for execution by the Inference Engine.  \n",
    "We also need to edit the `CMakeLists.txt` file to add any header file or library dependencies \n",
    "required to compile the CPU extension.  In the next sections, we will walk through and edit \n",
    "these files.\n",
    "\n",
    "#### Edit `ext_cosh.cpp`\n",
    "\n",
    "We will now edit the `ext_cosh.cpp` by walking through the code and making the necessary \n",
    "changes for the `cosh` custom layer along the way.\n",
    "\n",
    "1. Using the text editor, open the CPU extension source file `$CLWS/cl_cosh/user_ie_extensions/cpu/ext_cosh.cpp`.\n",
    "\n",
    "2. To implement the `cosh` function to efficiently execute in parallel, the code will use the parallel processing supported by the Inference Engine through the use of the Intel® Threading Building Blocks library.  To use the library, at the top we must include the header [`ie_parallel.hpp`](https://docs.openvinotoolkit.org/2019_R3.1/ie__parallel_8hpp.html) file by adding the `#include` line as shown below.\n",
    "\n",
    "    Before:\n",
    "\n",
    "    ```cpp\n",
    "    #include \"ext_base.hpp\"\n",
    "    #include <cmath>\n",
    "    ```\n",
    "\n",
    "    After:\n",
    "\n",
    "    ```cpp\n",
    "    #include \"ext_base.hpp\"\n",
    "    #include \"ie_parallel.hpp\"\n",
    "    #include <cmath>\n",
    "    ```\n",
    "\n",
    "3. The class `coshImp` implements the `cosh` custom layer and inherits from the extension layer base class `ExtLayerBase`.\n",
    "\n",
    "    ```cpp\n",
    "    class coshImpl: public ExtLayerBase {\n",
    "        public:\n",
    "    ```\n",
    "\n",
    "4. The `coshImpl` constructor is passed the `layer` object that it is associated with to provide access to any layer parameters that may be needed when implementing the specific instance of the custom layer.\n",
    "\n",
    "    ```cpp\n",
    "    explicit coshImpl(const CNNLayer* layer) {\n",
    "      try {\n",
    "        ...\n",
    "    ```\n",
    "\n",
    "5. The `coshImpl` constructor configures the input and output data layout for the custom layer by calling `addConfig()`.  In the template file, the line is commented-out and we will replace it to indicate that `layer` uses `DataConfigurator(ConfLayout::PLN)` (plain or linear) data for both input and output.\n",
    "\n",
    "    Before:\n",
    "\n",
    "    ```cpp\n",
    "    ...\n",
    "    // addConfig({DataConfigurator(ConfLayout::PLN), DataConfigurator(ConfLayout::PLN)}, {DataConfigurator(ConfLayout::PLN)});\n",
    "\n",
    "    ```\n",
    "\n",
    "    After:\n",
    "\n",
    "    ```cpp\n",
    "    addConfig(layer, { DataConfigurator(ConfLayout::PLN) }, { DataConfigurator(ConfLayout::PLN) });\n",
    "    ```\n",
    "\n",
    "6. The construct is now complete, catching and reporting certain exceptions that may have been thrown before exiting.\n",
    "\n",
    "    ```cpp\n",
    "      } catch (InferenceEngine::details::InferenceEngineException &ex) {\n",
    "        errorMsg = ex.what();\n",
    "      }\n",
    "    }\n",
    "    ```\n",
    "\n",
    "7. The `execute` method is overridden to implement the functionality of the `cosh` custom layer.  The `inputs` and `outputs` are the data buffers passed as [`Blob`](https://docs.openvinotoolkit.org/2019_R3.1/_docs_IE_DG_Memory_primitives.html) objects.  The template file will simply return `NOT_IMPLEMENTED` by default.  To calculate the `cosh` custom layer, we will replace the `execute` method with the code needed to calculate the `cosh` function in parallel using the [`parallel_for3d`](https://docs.openvinotoolkit.org/2019_R3.1/ie__parallel_8hpp.html) function.\n",
    "\n",
    "    Before:\n",
    "\n",
    "    ```cpp\n",
    "      StatusCode execute(std::vector<Blob::Ptr>& inputs, std::vector<Blob::Ptr>& outputs,\n",
    "        ResponseDesc *resp) noexcept override {\n",
    "        // Add here implementation for layer inference\n",
    "        // Examples of implementations you can find in Inference Engine tool samples/extensions folder\n",
    "        return NOT_IMPLEMENTED;\n",
    "    ```\n",
    "\n",
    "    After:\n",
    "    ```cpp\n",
    "      StatusCode execute(std::vector<Blob::Ptr>& inputs, std::vector<Blob::Ptr>& outputs,\n",
    "        ResponseDesc *resp) noexcept override {\n",
    "        // Add implementation for layer inference here\n",
    "        // Examples of implementations are in OpenVINO samples/extensions folder\n",
    "\n",
    "        // Get pointers to source and destination buffers\n",
    "        float* src_data = inputs[0]->buffer();\n",
    "        float* dst_data = outputs[0]->buffer();\n",
    "\n",
    "        // Get the dimensions from the input (output dimensions are the same)\n",
    "        SizeVector dims = inputs[0]->getTensorDesc().getDims();\n",
    "\n",
    "        // Get dimensions:N=Batch size, C=Number of Channels, H=Height, W=Width\n",
    "        int N = static_cast<int>((dims.size() > 0) ? dims[0] : 1);\n",
    "        int C = static_cast<int>((dims.size() > 1) ? dims[1] : 1);\n",
    "        int H = static_cast<int>((dims.size() > 2) ? dims[2] : 1);\n",
    "        int W = static_cast<int>((dims.size() > 3) ? dims[3] : 1);\n",
    "\n",
    "        // Perform (in parallel) the hyperbolic cosine given by: \n",
    "        //    cosh(x) = (e^x + e^-x)/2\n",
    "        parallel_for3d(N, C, H, [&](int b, int c, int h) {\n",
    "        // Fill output_sequences with -1\n",
    "        for (size_t ii = 0; ii < b*c; ii++) {\n",
    "          dst_data[ii] = (exp(src_data[ii]) + exp(-src_data[ii]))/2;\n",
    "        }\n",
    "      });\n",
    "    return OK;\n",
    "    }\n",
    "    ```\n",
    "\n",
    "#### Edit `CMakeLists.txt`\n",
    "\n",
    "Because the implementation of the `cosh` custom layer makes use of the parallel processing \n",
    "supported by the Inference Engine, we need to add the Intel® Threading Building Blocks \n",
    "dependency to `CMakeLists.txt` before compiling.  We will add paths to the header \n",
    "and library files and add the Intel® Threading Building Blocks library to the list of link libraries. \n",
    "We will also rename the `.so`.\n",
    "\n",
    "1. Using the text editor, open the CPU extension CMake file `$CLWS/cl_cosh/user_ie_extensions/cpu/CMakeLists.txt`.\n",
    "2. At the top, rename the `TARGET_NAME` so that the compiled library is named `libcosh_cpu_extension.so`:\n",
    "\n",
    "    Before:\n",
    "\n",
    "    ```cmake\n",
    "    set(TARGET_NAME \"user_cpu_extension\")\n",
    "    ```\n",
    "\n",
    "    After:\n",
    "    \n",
    "    ```cmake\n",
    "    set(TARGET_NAME \"cosh_cpu_extension\")\n",
    "    ```\n",
    "\n",
    "3. We modify the `include_directories` to add the header include path for the Intel® Threading Building Blocks library located in `/opt/intel/openvino/deployment_tools/inference_engine/external/tbb/include`:\n",
    "\n",
    "    Before:\n",
    "\n",
    "    ```cmake\n",
    "    include_directories (PRIVATE\n",
    "    ${CMAKE_CURRENT_SOURCE_DIR}/common\n",
    "    ${InferenceEngine_INCLUDE_DIRS}\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    After:\n",
    "    ```cmake\n",
    "    include_directories (PRIVATE\n",
    "    ${CMAKE_CURRENT_SOURCE_DIR}/common\n",
    "    ${InferenceEngine_INCLUDE_DIRS}\n",
    "    \"/opt/intel/openvino/deployment_tools/inference_engine/external/tbb/include\"\n",
    "    )\n",
    "    ```\n",
    "\n",
    "4. We add the `link_directories` with the path to the Intel® Threading Building Blocks library binaries at `/opt/intel/openvino/deployment_tools/inference_engine/external/tbb/lib`:\n",
    "\n",
    "    Before:\n",
    "\n",
    "    ```cmake\n",
    "    ...\n",
    "    #enable_omp()\n",
    "    ```\n",
    "\n",
    "    After:\n",
    "    ```cmake\n",
    "    ...\n",
    "    link_directories(\n",
    "    \"/opt/intel/openvino/deployment_tools/inference_engine/external/tbb/lib\"\n",
    "    )\n",
    "    #enable_omp()\n",
    "    ```\n",
    "\n",
    "5. Finally, we add the Intel® Threading Building Blocks library `tbb` to the list of link libraries in `target_link_libraries`:\n",
    "\n",
    "    Before:\n",
    "\n",
    "    ```cmake\n",
    "    target_link_libraries(${TARGET_NAME} ${InferenceEngine_LIBRARIES} ${intel_omp_lib})\n",
    "    ```\n",
    "\n",
    "    After:\n",
    "\n",
    "    ```cmake\n",
    "    target_link_libraries(${TARGET_NAME} ${InferenceEngine_LIBRARIES} ${intel_omp_lib} tbb)\n",
    "    ```\n",
    "\n",
    "### Compile the Extension Library\n",
    "\n",
    "To run the custom layer on the CPU during inference, the edited extension C++ source code \n",
    "must be compiled to create a `.so` shared library used by the Inference Engine. \n",
    "In the following steps, we will now compile the extension C++ library.\n",
    "\n",
    "1. First, we run the following commands to use CMake to setup for compiling:\n",
    "\n",
    "    ```bash\n",
    "    cd $CLWS/cl_cosh/user_ie_extensions/cpu\n",
    "    mkdir -p build\n",
    "    cd build\n",
    "    cmake ..\n",
    "    ```\n",
    "\n",
    "    The output will appear similar to:     \n",
    "\n",
    "    ```\n",
    "    -- Generating done\n",
    "    -- Build files have been written to: /home/<user>/cl_tutorial/cl_cosh/user_ie_extensions/cpu/build\n",
    "    ```\n",
    "\n",
    "2. The CPU extension library is now ready to be compiled.  Compile the library using the command:\n",
    "\n",
    "    ```bash\n",
    "    make -j $(nproc)\n",
    "    ```\n",
    "\n",
    "    The output will appear similar to: \n",
    "\n",
    "    ```\n",
    "    [100%] Linking CXX shared library libcosh_cpu_extension.so\n",
    "    [100%] Built target cosh_cpu_extension\n",
    "    ```\n",
    "\n",
    "Move to the next page to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Execute the Model with the Custom Layer\n",
    "\n",
    "### Using a C++ Sample\n",
    "\n",
    "To start on a C++ sample, we first need to build the C++ samples for use with the Inference\n",
    "Engine:\n",
    "\n",
    "```bash\n",
    "cd /opt/intel/openvino/deployment_tools/inference_engine/samples/\n",
    "./build_samples.sh\n",
    "```\n",
    "\n",
    "This will take a few minutes to compile all of the samples.\n",
    "\n",
    "Next, we will try running the C++ sample without including the `cosh` extension library to see \n",
    "the error describing the unsupported `cosh` operation using the command:  \n",
    "\n",
    "```bash\n",
    "~/inference_engine_samples_build/intel64/Release/classification_sample_async -i $CLT/pics/dog.bmp -m $CLWS/cl_ext_cosh/model.ckpt.xml -d CPU\n",
    "```\n",
    "\n",
    "The error output will be similar to:\n",
    "\n",
    "```\n",
    "[ ERROR ] Unsupported primitive of type: cosh name: ModCosh/cosh/Cosh\n",
    "```\n",
    "\n",
    "We will now run the command again, this time with the `cosh` extension library specified \n",
    "using the `-l $CLWS/cl_cosh/user_ie_extensions/cpu/build/libcosh_cpu_extension.so` option \n",
    "in the command:\n",
    "\n",
    "```bash\n",
    "~/inference_engine_samples_build/intel64/Release/classification_sample_async -i $CLT/pics/dog.bmp -m $CLWS/cl_ext_cosh/model.ckpt.xml -d CPU -l $CLWS/cl_cosh/user_ie_extensions/cpu/build/libcosh_cpu_extension.so\n",
    "```\n",
    "\n",
    "The output will appear similar to:\n",
    "\n",
    "```\n",
    "Image /home/<user>/cl_tutorial/OpenVINO-Custom-Layers/pics/dog.bmp\n",
    "\n",
    "classid probability\n",
    "------- -----------\n",
    "0       0.9308984  \n",
    "1       0.0691015\n",
    "\n",
    "total inference time: xx.xxxxxxx\n",
    "Average running time of one iteration: xx.xxxxxxx ms\n",
    "\n",
    "Throughput: xx.xxxxxxx FPS\n",
    "\n",
    "[ INFO ] Execution successful\n",
    "```\n",
    "\n",
    "### Using a Python Sample\n",
    "\n",
    "First, we will try running the Python sample without including the `cosh` extension library \n",
    "to see the error describing the unsupported `cosh` operation using the command:  \n",
    "\n",
    "```bash\n",
    "python /opt/intel/openvino/deployment_tools/inference_engine/samples/python_samples/classification_sample_async/classification_sample_async.py -i $CLT/pics/dog.bmp -m $CLWS/cl_ext_cosh/model.ckpt.xml -d CPU\n",
    "```\n",
    "\n",
    "The error output will be similar to:\n",
    "\n",
    "```\n",
    "[ INFO ] Loading network files:\n",
    "/home/<user>/cl_tutorial/tf_model/model.ckpt.xml\n",
    "/home/<user>/cl_tutorial/tf_model/model.ckpt.bin\n",
    "[ ERROR ] Following layers are not supported by the plugin for specified device CPU:\n",
    "ModCosh/cosh/Cosh, ModCosh/cosh_1/Cosh, ModCosh/cosh_2/Cosh\n",
    "[ ERROR ] Please try to specify cpu extensions library path in sample's command line parameters using -l or --cpu_extension command line argument\n",
    "```\n",
    "\n",
    "We will now run the command again, this time with the `cosh` extension library specified \n",
    "using the `-l $CLWS/cl_cosh/user_ie_extensions/cpu/build/libcosh_cpu_extension.so` option \n",
    "in the command:\n",
    "\n",
    "```bash\n",
    "python /opt/intel/openvino/deployment_tools/inference_engine/samples/python_samples/classification_sample_async/classification_sample_async.py -i $CLT/pics/dog.bmp -m $CLWS/cl_ext_cosh/model.ckpt.xml -l $CLWS/cl_cosh/user_ie_extensions/cpu/build/libcosh_cpu_extension.so -d CPU\n",
    "```\n",
    "\n",
    "The output will appear similar to:\n",
    "\n",
    "```\n",
    "Image /home/<user>/cl_tutorial/OpenVINO-Custom-Layers/pics/dog.bmp\n",
    "\n",
    "classid probability\n",
    "------- -----------\n",
    "0      0.9308984\n",
    "1      0.0691015\n",
    "```\n",
    "\n",
    "**Congratulations!** You have now implemented a custom layer with the Intel® Distribution of OpenVINO™ Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-6f2a60e5": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Feed an IR to the Inference Engine\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-6f2a60e5\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "Earlier in the course, you were focused on working with the Intermediate Representation (IR)\n",
    "models themselves, while mostly glossing over the use of the actual Inference Engine with\n",
    "the model.\n",
    "\n",
    "Here, you'll import the Python wrapper for the Inference Engine (IE), and practice using \n",
    "different IRs with it. You will first add each IR as an `IENetwork`, and check whether the layers \n",
    "of that network are supported by the classroom CPU.\n",
    "\n",
    "Since the classroom workspace is using an Intel CPU, you will also need to add a CPU\n",
    "extension to the `IECore`.\n",
    "\n",
    "Once you have verified all layers are supported (when the CPU extension is added),\n",
    "you will load the given model into the Inference Engine.\n",
    "\n",
    "Note that the `.xml` file of the IR should be given as an argument when running the script.\n",
    "\n",
    "To test your implementation, you should be able to successfully load each of the three IR\n",
    "model files we have been working with throughout the course so far, which you can find in the\n",
    "`/home/workspace/models` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-ceb2f99a": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Inference Requests\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-ceb2f99a\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "In the previous exercise, you loaded Intermediate Representations (IRs) into the Inference\n",
    "Engine. Now that we've covered some of the topics around requests, including the difference\n",
    "between synchronous and asynchronous requests, you'll add additional code to make\n",
    "inference requests to the Inference Engine.\n",
    "\n",
    "Given an `ExecutableNetwork` that is the IR loaded into the Inference Engine, your task is to:\n",
    "\n",
    "1. Perform a synchronous request\n",
    "2. Start an asynchronous request given an input image frame\n",
    "3. Wait for the asynchronous request to complete\n",
    "\n",
    "Note that we'll cover handling the results of the request shortly, so you don't need to worry\n",
    "about that just yet. This will get you practice with both types of requests with the Inference\n",
    "Engine.\n",
    "\n",
    "You will perform the above tasks within `inference.py`. This will take three arguments,\n",
    "one for the model, one for the test image, and the last for what type of inference request\n",
    "should be made.\n",
    "\n",
    "You can use `test.py` afterward to verify your code successfully makes inference requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-d44d77ce": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Integrate the Inference Engine in An Edge App\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-d44d77ce\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "You've come a long way from the first lesson where most of the code for working with\n",
    "the OpenVINO toolkit was happening in the background. You worked with pre-trained models,\n",
    "moved up to converting any trained model to an Intermediate Representation with the\n",
    "Model Optimizer, and even got the model loaded into the Inference Engine and began making\n",
    "inference requests.\n",
    "\n",
    "In this final exercise of this lesson, you'll close off the OpenVINO workflow by extracting\n",
    "the results of the inference request, and then integrating the Inference Engine into an existing\n",
    "application. You'll still be given some of the overall application infrastructure, as more that of\n",
    "will come in the next lesson, but all of that is outside of OpenVINO itself.\n",
    "\n",
    "You will also add code allowing you to try out various confidence thresholds with the model,\n",
    "as well as changing the visual look of the output, like bounding box colors.\n",
    "\n",
    "Now, it's up to you which exact model you want to use here, although you are able to just\n",
    "re-use the model you converted with TensorFlow before for an easy bounding box dectector.\n",
    "\n",
    "Note that this application will run with a video instead of just images like we've done before.\n",
    "\n",
    "So, your tasks are to:\n",
    "\n",
    "1. Convert a bounding box model to an IR with the Model Optimizer.\n",
    "2. Pre-process the model as necessary.\n",
    "3. Use an async request to perform inference on each video frame.\n",
    "4. Extract the results from the inference request.\n",
    "5. Add code to make the requests and feed back the results within the application.\n",
    "6. Perform any necessary post-processing steps to get the bounding boxes.\n",
    "7. Add a command line argument to allow for different confidence thresholds for the model.\n",
    "8. Add a command line argument to allow for different bounding box colors for the output.\n",
    "9. Correctly utilize the command line arguments in #3 and #4 within the application.\n",
    "\n",
    "When you are done, feed your model to `app.py`, and it will generate `out.mp4`, which you\n",
    "can download and view. *Note that this app will take a little bit longer to run.* Also, if you need\n",
    "to re-run inference, delete the `out.mp4` file first.\n",
    "\n",
    "You only need to feed the model with `-m` before adding the customization; you should set\n",
    "defaults for any additional arguments you add for the color and confidence so that the user\n",
    "does not always need to specify them.\n",
    "\n",
    "```bash\n",
    "python app.py -m {your-model-path.xml}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-5de618db": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Handling Input Streams\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-5de618db\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "It's time to really get in the think of things for running your app at the edge. Being able to\n",
    "appropriately handle an input stream is a big part of having a working AI or computer vision\n",
    "application. \n",
    "\n",
    "In your case, you will be implementing a function that can handle camera, video or webcam\n",
    "data as input. While unfortunately the classroom workspace won't allow for webcam usage,\n",
    "you can also try that portion of your code out on your local machine if you have a webcam\n",
    "available.\n",
    "\n",
    "As such, the tests here will focus on using a camera image or a video file. You will not need to\n",
    "perform any inference on the input frames, but you will need to do a few other image\n",
    "processing techniques to show you have some of the basics of OpenCV down.\n",
    "\n",
    "Your tasks are to:\n",
    "\n",
    "1. Implement a function that can handle camera image, video file or webcam inputs\n",
    "2. Use `cv2.VideoCapture()` and open the capture stream\n",
    "3. Re-size the frame to 100x100\n",
    "4. Add Canny Edge Detection to the frame with min & max values of 100 and 200, respectively\n",
    "5. Save down the image or video output\n",
    "6. Close the stream and any windows at the end of the application\n",
    "\n",
    "You won't be able to test a webcam input in the workspace unfortunately, but you can use\n",
    "the included video and test image to test your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-4fb9f776": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Processing Model Outputs\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-4fb9f776\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "Let's say you have a cat and two dogs at your house. \n",
    "\n",
    "If both dogs are in a room together, they are best buds, and everything is going well.\n",
    "\n",
    "If the cat and dog #1 are in a room together, they are also good friends, and everything is fine.\n",
    "\n",
    "However, if the cat and dog #2 are in a room together, they don't get along, and you may need\n",
    "to either pull them apart, or at least play a pre-recorded message from your smart speaker\n",
    "to tell them to cut it out.\n",
    "\n",
    "In this exercise, you'll receive a video where some combination or the cat and dogs may be\n",
    "in view. You also will have an IR that is able to determine which of these, if any, are on screen.\n",
    "\n",
    "While the best model for this is likely an object detection model that can identify different\n",
    "breeds, I have provided you with a very basic (and overfit) model that will return three classes,\n",
    "one for one or less pets on screen, one for the bad combination of the cat and dog #2, and\n",
    "one for the fine combination of the cat and dog #1. This is within the exercise directory - `model.xml`.\n",
    "\n",
    "It is up to you to add code that will print to the terminal anytime the bad combination of the \n",
    "cat and dog #2 are detected together. **Note**: It's important to consider whether you really\n",
    "want to output a warning *every single time* both pets are on-screen - is your warning helpful\n",
    "if it re-starts every 30th of a second, with a video at 30 fps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!--\n",
    "%%ulab_page_divider\n",
    "--><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-66f8bc80": {
       "bashCommand": "source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "primary",
       "text": "SOURCE ENV",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "# Server Communications\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-66f8bc80\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "In this exercise, you will practice showing off your new server communication skills\n",
    "for sending statistics over MQTT and images with FFMPEG.\n",
    "\n",
    "The application itself is already built and able to perform inference, and a node server is set\n",
    "up for you to use. The main node server is already fully ready to receive communications from\n",
    "MQTT and FFMPEG. The MQTT node server is fully configured as well. Lastly, the ffserver is \n",
    "already configured for FFMPEG too.\n",
    "\n",
    "The current application simply performs inference on a frame, gathers some statistics, and then \n",
    "continues onward to the next frame. \n",
    "\n",
    "## Tasks\n",
    "\n",
    "Your tasks are to:\n",
    "\n",
    "- Add any code for MQTT to the project so that the node server receives the calculated stats\n",
    "  - This includes importing the relevant Python library\n",
    "  - Setting IP address and port\n",
    "  - Connecting to the MQTT client\n",
    "  - Publishing the calculated statistics to the client\n",
    "- Send the output frame (**not** the input image, but the processed output) to the ffserver\n",
    "\n",
    "## Additional Information\n",
    "\n",
    "Note: Since you are given the MQTT Broker Server and Node Server for the UI, you need \n",
    "certain information to correctly configure, publish and subscribe with MQTT.\n",
    "- The MQTT port to use is 3001 - the classroom workspace only allows ports 3000-3009\n",
    "- The topics that the UI Node Server is listening to are \"class\" and \"speedometer\"\n",
    "- The Node Server will attempt to extract information from any JSON received from the MQTT server with the keys \"class_names\" and \"speed\"\n",
    "\n",
    "## Running the App\n",
    "\n",
    "First, get the MQTT broker and UI installed.\n",
    "\n",
    "- `cd webservice/server`\n",
    "- `npm install`\n",
    "- When complete, `cd ../ui`\n",
    "- And again, `npm install`\n",
    "\n",
    "You will need *four* separate terminal windows open in order to see the results. The steps\n",
    "below should be done in a different terminal based on number. You can open a new terminal\n",
    "in the workspace in the upper left (File>>New>>Terminal).\n",
    "\n",
    "1. Get the MQTT broker installed and running.\n",
    "  - `cd webservice/server/node-server`\n",
    "  - `node ./server.js`\n",
    "  - You should see a message that `Mosca server started.`.\n",
    "2. Get the UI Node Server running.\n",
    "  - `cd webservice/ui`\n",
    "  - `npm run dev`\n",
    "  - After a few seconds, you should see `webpack: Compiled successfully.`\n",
    "3. Start the ffserver\n",
    "  - `sudo ffserver -f ./ffmpeg/server.conf`\n",
    "4. Start the actual application. \n",
    "  - First, you need to source the environment for OpenVINO *in the new terminal*:\n",
    "    - `source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5`\n",
    "  - To run the app, I'll give you two items to pipe in with `ffmpeg` here, with the rest up to you:\n",
    "    - `-video_size 1280x720`\n",
    "    - `-i - http://0.0.0.0:3004/fac.ffm`\n",
    "\n",
    "Your app should begin running, and you should also see the MQTT broker server noting\n",
    "information getting published.\n",
    "\n",
    "In order to view the output, click on the \"Open App\" button below in the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "ulab": {
     "buttons": {
      "ulab-button-2c5c842f": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": true,
       "runInBackground": false,
       "style": "primary",
       "text": "OPEN APP",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "<button id=\"ulab-button-2c5c842f\" class=\"ulab-btn--primary\"></button>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "ulab_nb_type": "guided"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
